# -*- coding: utf-8 -*-
"""KEWLONA_INDICATOR

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11JvO_ZTZqwav45p6qF3uQvPgkRt_CBV6
"""

!pip install osmnx

import geopandas as gpd
import osmnx as ox
import networkx as nx
import matplotlib.pyplot as plt
import os

# Function to extract OSM indicators
def extract_osm_indicators(G):
    # 1. Intersection density
    intersection_count = sum(1 for _, degree in G.degree() if degree > 2)
    intersection_density = intersection_count / G.graph.get('area', 1)

    # 2. Circuity
    edge_lengths = [d['length'] for _, _, d in G.edges(data=True)]
    total_edge_length = sum(edge_lengths)
    straight_line_distances = [
        ox.distance.great_circle(G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x'])
        for u, v in G.edges()
    ]
    total_straight_line_distance = sum(straight_line_distances)
    circuity = total_edge_length / total_straight_line_distance if total_straight_line_distance else 0

    # 3. Street segment count
    street_segment_count = len(G.edges())

    # 4. Street length total
    street_length_total = total_edge_length

    # 5. Streets per node
    streets_per_node = [degree for _, degree in G.degree()]

    # 6. Edge length
    edge_length = total_edge_length

    # 7. Street density
    street_density = street_length_total / G.graph.get('area', 1)

    # 8. Edge density
    edge_density = edge_length / G.graph.get('area', 1)

    # 9. Node density
    node_density = len(G.nodes()) / G.graph.get('area', 1)

    # 10. 1, 2, 4-way intersections
    one_way_intersections = sum(1 for _, degree in G.degree() if degree == 1)
    two_way_intersections = sum(1 for _, degree in G.degree() if degree == 2)
    four_way_intersections = sum(1 for _, degree in G.degree() if degree == 4)

    # 11. Betweenness centrality
    betweenness_centralities = nx.betweenness_centrality(G, weight='length')
    avg_betweenness_centrality = sum(betweenness_centralities.values()) / len(betweenness_centralities) if betweenness_centralities else 0

    # 12. Closeness centrality (within centrality)
    closeness_centralities = nx.closeness_centrality(G, distance='length')
    avg_closeness_centrality = sum(closeness_centralities.values()) / len(closeness_centralities) if closeness_centralities else 0

    return {
        'intersection_density': intersection_density,
        'circuity': circuity,
        'street_segment_count': street_segment_count,
        'street_length_total': street_length_total,
        'streets_per_node_avg': sum(streets_per_node) / len(streets_per_node) if streets_per_node else 0,
        'edge_length': edge_length,
        'street_density': street_density,
        'edge_density': edge_density,
        'node_density': node_density,
        'one_way_intersections': one_way_intersections,
        'two_way_intersections': two_way_intersections,
        'four_way_intersections': four_way_intersections,
        'avg_betweenness_centrality': avg_betweenness_centrality,
        'avg_closeness_centrality': avg_closeness_centrality
    }

def calculate_indicators(polygon):
    try:
        # Download the street network within the polygon
        G = ox.graph_from_polygon(polygon, network_type='all', simplify=True, retain_all=True)

        # Check if the graph is empty or has no edges
        if len(G) == 0 or len(G.edges) == 0:
            raise ValueError("Graph is empty or has no edges")

        # Set the 'area' attribute in the graph object
        G.graph['area'] = polygon.area

        # Calculate indicators
        indicators = extract_osm_indicators(G)

    except (ValueError, nx.NetworkXPointlessConcept):
        # No graph nodes found within the polygon or graph is empty, return default values for the indicators
        indicators = {key: 0 for key in extract_osm_indicators(nx.Graph()).keys()}
        G = None

    return indicators, G

# Read the shapefiles
shapefile_path = '/content/kelowna_all__grids_indicators.shp'
kelowna_grids = gpd.read_file(shapefile_path)

# Reproject the shapefiles to WGS 84 (EPSG:4326)
kelowna_grids = kelowna_grids.to_crs(epsg=4326)

# Create a new GeoDataFrame to store the results
kelowna_grids_indicators = kelowna_grids.copy()

# Define the directory to save the road networks
output_dir = 'kelowna_road_networks'
os.makedirs(output_dir, exist_ok=True)

# Iterate through each box in the grid and calculate indicators
for index, row in kelowna_grids.iterrows():
    polygon = row['geometry']
    indicators, G = calculate_indicators(polygon)

    # Add the indicators to the new GeoDataFrame
    for key, value in indicators.items():
        kelowna_grids_indicators.loc[index, key] = value

    # Save the network plot if the graph is not empty
    if G is not None:
        fig, ax = ox.plot_graph(G, show=False, close=False)
        plt.savefig(os.path.join(output_dir, f'road_network_{index}.png'))
        plt.close(fig)

# Save the new GeoDataFrame as a shapefile
kelowna_grids_indicators.to_file('kelowna_all__grids_indicators.shp')

!pip install libpysal

import geopandas as gpd

# Load shapefiles
origins = gpd.read_file('/content/od/origin.shp')
destinations = gpd.read_file('/content/od/destination.shp')

# Ensure the GeoDataFrames have a CRS (assuming EPSG:4326 if not defined)
origins = origins.set_crs(epsg=4326, allow_override=True)
destinations = destinations.set_crs(epsg=4326, allow_override=True)

# Step 1: Install necessary packages
!pip install geopandas fiona



# Step 3: Set the SHAPE_RESTORE_SHX configuration option
import os
os.environ["SHAPE_RESTORE_SHX"] = "YES"

# Step 4: Read the shapefile using GeoPandas
import geopandas as gpd

# Assuming the main file is indicator.shp and it has been uploaded correctly
shapefile_path = "/content/indicator.shp"
gdf = gpd.read_file(shapefile_path)

# Display the first few rows of the GeoDataFrame
print(gdf.head())

# Step 5: Add a new unique identifier column
gdf['unique_id'] = range(1, len(gdf) + 1)

# Display the first few rows with the new unique identifier column
print(gdf.head())

import pandas as pd
import geopandas as gpd

# Load the shapefile (indicator.shp)
shapefile_path = '/content/indicator.shp'  # Replace with your actual path
gdf_shapefile = gpd.read_file(shapefile_path)

# Load the CSV file (final_data1.csv)
csv_path = '/content/final_data1.csv'  # Replace with your actual path
df_csv = pd.read_csv(csv_path)

# Display the columns to understand the structure
print("Shapefile columns:")
print(gdf_shapefile.columns)

print("\nCSV columns:")
print(df_csv.columns)

# Create a DataFrame from the shapefile's attributes (excluding geometry)
df_shapefile_attributes = gdf_shapefile.drop(columns='geometry')

# Perform the merge based on 'origin_id'
# Replicate shapefile attributes for each corresponding origin_id in the CSV
merged_data = df_csv.merge(df_shapefile_attributes, on='origin_id', how='left')

# Save the merged DataFrame to a new CSV file
merged_csv_path = '/content/merged_final_data.csv'  # Replace with your desired path
merged_data.to_csv(merged_csv_path, index=False)

print(f"\nMerged data saved to {merged_csv_path}")

# Display the head of the GeoDataFrames
print("Origins GeoDataFrame head:")
print(gdf_origins.head())

print("\nDestinations GeoDataFrame head:")
print(gdf_destinations.head())

!pip install osmnx

import geopandas as gpd
import osmnx as ox
import networkx as nx
import matplotlib.pyplot as plt
import os

# Function to extract OSM indicators
def extract_osm_indicators(G):
    # 1. Intersection density
    intersection_count = sum(1 for _, degree in G.degree() if degree > 2)
    intersection_density = intersection_count / G.graph.get('area', 1)

    # 2. Circuity
    edge_lengths = [d['length'] for _, _, d in G.edges(data=True)]
    total_edge_length = sum(edge_lengths)
    straight_line_distances = [
        ox.distance.great_circle_vec(G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x'])
        for u, v in G.edges()
    ]
    total_straight_line_distance = sum(straight_line_distances)
    circuity = total_edge_length / total_straight_line_distance if total_straight_line_distance else 0

    # 3. Street segment count
    street_segment_count = len(G.edges())

    # 4. Street length total
    street_length_total = total_edge_length

    # 5. Streets per node
    streets_per_node = [degree for _, degree in G.degree()]

    # 6. Edge length
    edge_length = total_edge_length

    # 7. Street density
    street_density = street_length_total / G.graph.get('area', 1)

    # 8. Edge density
    edge_density = edge_length / G.graph.get('area', 1)

    # 9. Node density
    node_density = len(G.nodes()) / G.graph.get('area', 1)

    # 10. 1, 2, 4-way intersections
    one_way_intersections = sum(1 for _, degree in G.degree() if degree == 1)
    two_way_intersections = sum(1 for _, degree in G.degree() if degree == 2)
    four_way_intersections = sum(1 for _, degree in G.degree() if degree == 4)

    # 11. Betweenness centrality
    betweenness_centralities = nx.betweenness_centrality(G, weight='length')
    avg_betweenness_centrality = sum(betweenness_centralities.values()) / len(betweenness_centralities) if betweenness_centralities else 0

    # 12. Closeness centrality
    closeness_centralities = nx.closeness_centrality(G, distance='length')
    avg_closeness_centrality = sum(closeness_centralities.values()) / len(closeness_centralities) if closeness_centralities else 0

    return {
        'intersection_density': intersection_density,
        'circuity': circuity,
        'street_segment_count': street_segment_count,
        'street_length_total': street_length_total,
        'streets_per_node_avg': sum(streets_per_node) / len(streets_per_node) if streets_per_node else 0,
        'edge_length': edge_length,
        'street_density': street_density,
        'edge_density': edge_density,
        'node_density': node_density,
        'one_way_intersections': one_way_intersections,
        'two_way_intersections': two_way_intersections,
        'four_way_intersections': four_way_intersections,
        'avg_betweenness_centrality': avg_betweenness_centrality,
        'avg_closeness_centrality': avg_closeness_centrality
    }

def calculate_indicators(polygon):
    try:
        # Download the street network within the polygon
        G = ox.graph_from_polygon(polygon, network_type='all', simplify=True, retain_all=True)

        # Check if the graph is empty or has no edges
        if len(G) == 0 or len(G.edges) == 0:
            raise ValueError("Graph is empty or has no edges")

        # Set the 'area' attribute in the graph object
        G.graph['area'] = polygon.area

        # Calculate indicators
        indicators = extract_osm_indicators(G)

    except (ValueError, nx.NetworkXPointlessConcept):
        # No graph nodes found within the polygon or graph is empty, return default values for the indicators
        indicators = {key: 0 for key in extract_osm_indicators(nx.Graph()).keys()}
        G = None

    return indicators, G

# Read the shapefiles
shapefile_path = '/content/1km.shp'
kelowna_grids = gpd.read_file(shapefile_path)

# Reproject the shapefiles to WGS 84 (EPSG:4326)
kelowna_grids = kelowna_grids.to_crs(epsg=4326)

# Create a new GeoDataFrame to store the results
kelowna_grids_indicators = kelowna_grids.copy()

# Define the directory to save the road networks
output_dir = 'kelowna_road_networks'
os.makedirs(output_dir, exist_ok=True)

# Iterate through each box in the grid and calculate indicators
for index, row in kelowna_grids.iterrows():
    polygon = row['geometry']
    indicators, G = calculate_indicators(polygon)

    # Add the indicators to the new GeoDataFrame
    for key, value in indicators.items():
        kelowna_grids_indicators.loc[index, key] = value

    # Save the network plot if the graph is not empty
    if G is not None:
        fig, ax = ox.plot_graph(G, show=False, close=False)
        plt.savefig(os.path.join(output_dir, f'road_network_{index}.png'))
        plt.close(fig)

# Save the new GeoDataFrame as a shapefile
kelowna_grids_indicators.to_file('kelowna_all__grids_indicators.shp')

!pip install libpysal

import geopandas as gpd
import pandas as pd
import random
from shapely.geometry import Polygon, Point, LineString
from libpysal.weights import Queen

def generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv):
    """
    Generate Origin-Destination (OD) pairs within a 5 km buffer from randomly selected 1 km grids in Kelowna.

    Parameters:
    - grid_shapefile (str): Path to the 1 km grid shapefile.
    - buffer_distance_km (int): Buffer distance in kilometers.
    - output_csv (str): Output CSV file path to save the OD pairs.

    Returns:
    - pd.DataFrame: DataFrame containing the OD pairs.
    """

    # Load the 1 km grid shapefile
    grids = gpd.read_file(grid_shapefile)

    # Ensure the CRS is in meters
    if grids.crs != 'EPSG:26910':  # Replace with the appropriate UTM zone if needed
        grids = grids.to_crs(epsg=26910)

    # Create a contiguity matrix using the Queen's method
    w = Queen.from_dataframe(grids)

    # Initialize list to store OD pairs
    od_pairs = []

    # Generate OD pairs within the buffers
    for i, origin in grids.iterrows():
        # Create a buffer of 5 km around the origin grid
        buffer = origin.geometry.buffer(buffer_distance_km * 1000)

        # Find neighbors within the buffer
        neighbor_indices = w.neighbors[i]
        neighbor_grids = grids.iloc[neighbor_indices]

        for j, destination in neighbor_grids.iterrows():
            if i != j:  # Ensure the origin and destination are not the same
                od_pairs.append({
                    'origin_id': origin['FID'],  # Using 'FID' as the origin identifier
                    'destination_id': destination['FID'],  # Using 'FID' as the destination identifier
                    'origin_geom': origin.geometry,
                    'destination_geom': destination.geometry
                })

    # Create a DataFrame to store the OD pairs
    od_pairs_df = pd.DataFrame(od_pairs)

    # Save the OD pairs to a CSV file
    od_pairs_df.to_csv(output_csv, index=False)

    return od_pairs_df

# Parameters
grid_shapefile = '/content/kelowna_all__grids_indicators.shp'  # Path to the 1 km grid shapefile
buffer_distance_km = 5  # Buffer distance in kilometers
output_csv = 'kelowna_od_pairs.csv'  # Output CSV file path

# Generate OD pairs
od_pairs_df = generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv)

print(f"Generated {len(od_pairs_df)} OD pairs within 5 km buffers from grids.")

# Visualize the OD pairs with contiguity
import matplotlib.pyplot as plt

# Create GeoDataFrames from the OD pairs DataFrame
od_pairs_gdf = gpd.GeoDataFrame(od_pairs_df, geometry='origin_geom')
od_pairs_gdf['destination_geom'] = gpd.GeoSeries(od_pairs_df['destination_geom'])

# Plot the grid and OD pairs
fig, ax = plt.subplots(figsize=(15, 15))
grids.plot(ax=ax, color='lightgrey', edgecolor='grey')

# Plotting destinations
for i, row in od_pairs_gdf.iterrows():
    x = [row['origin_geom'].centroid.x, row['destination_geom'].centroid.x]
    y = [row['origin_geom'].centroid.y, row['destination_geom'].centroid.y]
    ax.plot(x, y, color='red')

# Plotting contiguity lines
for i, row in od_pairs_gdf.iterrows():
    origin_point = row['origin_geom'].centroid
    destination_point = row['destination_geom'].centroid
    line = LineString([origin_point, destination_point])
    ax.add_patch(Polygon(line.buffer(0.001).exterior, color='blue', alpha=0.5))

plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Origin-Destination Pairs with Contiguity (Queen\'s Method) within 5 km Buffers')
plt.show()

import geopandas as gpd
import pandas as pd
from shapely.geometry import Point

from math import cos,pi
from shapely.geometry import box

# Load the shapefile
grid = gpd.read_file("/kewlona1km.shp")

# Ensure each grid cell has a unique identifier starting from 1
grid['unique_id'] = range(1, len(grid) + 1)

# Calculate centroids for each grid cell
grid['centroid'] = grid.geometry.centroid

# Latitude and longitude approximations for a 4 km buffer
buffer_lat = 4 / 111  # Approx. 0.036 degrees latitude
buffer_lon = 4 / (111 * abs(cos(49.8875 * pi / 180)))  # Approx. 0.057 degrees longitude for Kelowna

# Create OD pairs
od_pairs = []

for origin_idx, origin in grid.iterrows():
    origin_centroid = origin['centroid']

    # Create a rectangular buffer of 4 km (approx.) around the origin centroid
    minx = origin_centroid.x - buffer_lon
    maxx = origin_centroid.x + buffer_lon
    miny = origin_centroid.y - buffer_lat
    maxy = origin_centroid.y + buffer_lat
    buffer_box = box(minx, miny, maxx, maxy)

    # Find all grid cells whose centroid is within the buffer
    destinations_within_buffer = grid[grid.centroid.within(buffer_box)]

    for destination_idx, destination in destinations_within_buffer.iterrows():
        dest_centroid = destination['centroid']

        od_pairs.append({
            'origin_id': origin['unique_id'],  # Using 'unique_id' as the origin identifier
            'destination_id': destination['unique_id'],  # Using 'unique_id' as the destination identifier
            'OriginLat': origin_centroid.y,
            'OriginLon': origin_centroid.x,
            'DestLat': dest_centroid.y,
            'DestLon': dest_centroid.x
        })

# Convert the list of OD pairs to a DataFrame
od_pairs_df = pd.DataFrame(od_pairs)

# Define the path to save the CSV file
csv_path = 'od_pairs.csv'

# Save the DataFrame as a CSV file
od_pairs_df.to_csv(csv_path, index=False)

print(f"OD pairs saved to {csv_path}")

files.download('od_pairs.csv')

import geopandas as gpd
import pandas as pd
import random
from shapely.geometry import Polygon, Point, LineString

def generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv):
    """
    Generate Origin-Destination (OD) pairs within a 5 km buffer from randomly selected 1 km grids in Kelowna.

    Parameters:
    - grid_shapefile (str): Path to the 1 km grid shapefile.
    - buffer_distance_km (int): Buffer distance in kilometers.
    - output_csv (str): Output CSV file path to save the OD pairs.

    Returns:
    - pd.DataFrame: DataFrame containing the OD pairs.
    """

    # Load the 1 km grid shapefile
    grids = gpd.read_file(grid_shapefile)

    # Ensure the CRS is in meters
    if grids.crs != 'EPSG:26910':  # Replace with the appropriate UTM zone if needed
        grids = grids.to_crs(epsg=26910)

    # Initialize list to store OD pairs
    od_pairs = []

    # Generate OD pairs within the buffers
    for i, origin in grids.iterrows():
        # Create a buffer of 5 km around the origin grid
        buffer = origin.geometry.buffer(buffer_distance_km * 1000)

        # Find neighbors using the Rook's method
        neighbor_grids = grids[grids.geometry.touches(buffer)]

        for j, destination in neighbor_grids.iterrows():
            if i != j:  # Ensure the origin and destination are not the same
                od_pairs.append({
                    'origin_id': origin['FID'],  # Using 'FID' as the origin identifier
                    'destination_id': destination['FID'],  # Using 'FID' as the destination identifier
                    'origin_geom': origin.geometry,
                    'destination_geom': destination.geometry
                })

    # Create a DataFrame to store the OD pairs
    od_pairs_df = pd.DataFrame(od_pairs)

    # Save the OD pairs to a CSV file
    od_pairs_df.to_csv(output_csv, index=False)

    return od_pairs_df

# Parameters
grid_shapefile = '/content/kelowna_all__grids_indicators.shp'  # Path to the 1 km grid shapefile
buffer_distance_km = 5  # Buffer distance in kilometers
output_csv = 'kelowna_od_pairs5km.csv'  # Output CSV file path

# Generate OD pairs
od_pairs_df = generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv)

print(f"Generated {len(od_pairs_df)} OD pairs within 5 km buffers from grids.")

# Visualize the OD pairs with contiguity
import matplotlib.pyplot as plt

# Create GeoDataFrames from the OD pairs DataFrame
od_pairs_gdf = gpd.GeoDataFrame(od_pairs_df, geometry='origin_geom')
od_pairs_gdf['destination_geom'] = gpd.GeoSeries(od_pairs_df['destination_geom'])

# Plot the grid and OD pairs
fig, ax = plt.subplots(figsize=(15, 15))
grids.plot(ax=ax, color='lightgrey', edgecolor='grey')

# Plotting destinations
for i, row in od_pairs_gdf.iterrows():
    x = [row['origin_geom'].centroid.x, row['destination_geom'].centroid.x]
    y = [row['origin_geom'].centroid.y, row['destination_geom'].centroid.y]
    ax.plot(x, y, color='red')

plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Origin-Destination Pairs with Contiguity (Rook\'s Method) within 5 km Buffers')
plt.show()

import geopandas as gpd

# Load the shapefile into a GeoDataFrame
gdf = gpd.read_file('/content/kelowna_all__grids_indicators.shp')

# Display the attribute table
print(gdf.head())
gdf

import pandas as pd

import geopandas as gpd

import geopandas as gpd
import pandas as pd

# Load the shapefile
grid = gpd.read_file('/content/1km.shp')

# Reproject to a projected CRS (e.g., EPSG: 3857) for accurate centroid and buffer calculations
grid_projected = grid.to_crs(epsg=3857)

# Ensure each grid cell has a unique identifier starting from 1
grid_projected['unique_id'] = range(1, len(grid_projected) + 1)

# Calculate centroids for each grid cell in the projected CRS
grid_projected['centroid'] = grid_projected.geometry.centroid

# Reproject centroids back to WGS 84 (EPSG: 4326) for API use
grid_projected['centroid_wgs84'] = grid_projected['centroid'].to_crs(epsg=4326)

# Create OD pairs
od_pairs = []

for origin_idx, origin in grid_projected.iterrows():
    origin_centroid = origin['centroid']

    # Create a buffer of 5km around the origin centroid
    buffer = origin_centroid.buffer(5000)  # Buffer in meters (5km)

    # Find all grid cells whose centroid is within the buffer
    destinations_within_buffer = grid_projected[grid_projected.centroid.within(buffer)]

    for destination_idx, destination in destinations_within_buffer.iterrows():
        dest_centroid = destination['centroid']
        dest_centroid_wgs84 = destination['centroid_wgs84']

        od_pairs.append({
            'origin_id': origin['unique_id'],
            'destination_id': destination['unique_id'],
            'OriginLat': origin['centroid_wgs84'].y,
            'OriginLon': origin['centroid_wgs84'].x,
            'DestLat': dest_centroid_wgs84.y,
            'DestLon': dest_centroid_wgs84.x
        })

# Convert the list of OD pairs to a DataFrame
od_pairs_df = pd.DataFrame(od_pairs)

# Print the number of OD pairs
print(f"Total number of OD pairs: {len(od_pairs_df)}")

# Define the path to save the CSV file
csv_path = 'od_pairs.csv'

# Save the DataFrame as a CSV file
od_pairs_df.to_csv(csv_path, index=False)

print(f"OD pairs saved to {csv_path}")

import geopandas as gpd

# Define the path to the shapefile
shapefile_path = '/content/kelowna_all__grids_indicators.shp'

# Read the shapefile
grid_data = gpd.read_file(shapefile_path)

# Display the attribute table
print(grid_data.head())

import geopandas as gpd

# Load the shapefile
shapefile_path = '/content/1km.shp'  # Adjust the path if necessary
gdf = gpd.read_file(shapefile_path)

# Display the projection system (CRS)
print(gdf.crs)

!pip install googlemaps

# # Install necessary libraries if not already installed
# !pip install geopandas pandas

import geopandas as gpd
import pandas as pd

def generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv):
    """
    Generate Origin-Destination (OD) pairs within a buffer distance from grids, considering car and transit travel modes.

    Parameters:
    - grid_shapefile (str): Path to the grid shapefile.
    - buffer_distance_km (int): Buffer distance in kilometers.
    - output_csv (str): Output CSV file path to save the OD pairs.

    Returns:
    - pd.DataFrame: DataFrame containing the OD pairs with travel mode ('car' or 'transit').
    """

    # Load the grid shapefile
    grids = gpd.read_file(grid_shapefile)

    # Ensure the CRS is WGS 84
    if grids.crs != 'EPSG:4326':  # EPSG:4326 is the code for WGS 84
        grids = grids.to_crs(epsg=4326)

    # Add unique IDs to the grids
    grids['Id'] = range(1, len(grids) + 1)

    # Initialize list to store OD pairs
    od_pairs = []

    # Generate OD pairs within the buffers for car and transit
    for travel_mode in ['car', 'transit']:
        for i, origin in grids.iterrows():
            # Create a buffer of buffer_distance_km km around the origin grid
            buffer = origin.geometry.buffer(buffer_distance_km / 111.32)  # Approximate degrees per km at the equator

            for j, destination in grids.iterrows():
                if i != j and buffer.contains(destination.geometry):  # Ensure the origin and destination are not the same
                    # Calculate centroid coordinates of origin and destination
                    origin_centroid = origin.geometry.centroid
                    dest_centroid = destination.geometry.centroid

                    # Append OD pair with centroid coordinates and travel mode
                    od_pairs.append({
                        'origin_id': origin['Id'],  # Using 'Id' as the origin identifier
                        'destination_id': destination['Id'],  # Using 'Id' as the destination identifier
                        'OriginLat': float(origin_centroid.y),
                        'OriginLon': float(origin_centroid.x),
                        'DestLat': float(dest_centroid.y),
                        'DestLon': float(dest_centroid.x),
                        'travel_mode': travel_mode
                    })

    # Create a DataFrame to store the OD pairs
    od_pairs_df = pd.DataFrame(od_pairs)

    # Save the OD pairs to a CSV file
    od_pairs_df.to_csv(output_csv, index=False)

    return od_pairs_df

# Parameters
grid_shapefile = '/kewlona1km.shp'  # Path to your grid shapefile in Colab
buffer_distance_km = 4  # Buffer distance in kilometers
output_csv = '/content/kelowna_od_pairs_car_transit.csv'  # Output CSV file path

# Generate OD pairs for car and transit
od_pairs_df = generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv)

print(f"Generated {len(od_pairs_df)} OD pairs (considering car and transit) within {buffer_distance_km} km buffers from grids.")



files.download('kelowna_od_pairs_car_transit.csv')

import geopandas as gpd
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Define the path to the shapefile
shapefile_path = '/content/kelowna_all__grids_indicators.shp'

# Read the shapefile
grid_data = gpd.read_file(shapefile_path)

# Select numerical columns for correlation analysis
numerical_columns = grid_data.select_dtypes(include=['int', 'float'])

# Calculate the correlation matrix
correlation_matrix = numerical_columns.corr()

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show()

from google.colab import drive  # Mount your Drive if needed (see previous responses)
from geopandas import read_file
import folium

# Replace with your file path (Colab storage or mounted Drive)
file_path = '/content/kelowna_all__grids_indicators.shp'

# Create a base map (replace with your desired center coordinates)
latitude = 49.884
longitude = -119.569
m = folium.Map(location=[latitude, longitude], zoom_start=12)

# Read the shapefile data
data = read_file(file_path)

# Convert GeoPandas data to GeoJSON
geojson = data.to_json()

# Add GeoJSON data to the map with a name
folium.GeoJson(data=geojson, name='Kelowna Grids Indicators').add_to(m)

# Display the map
m

import geopandas as gpd
import pandas as pd
import osmnx as ox


def generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv):
    """
    Generate Origin-Destination (OD) pairs within a buffer distance from grids, considering car and transit travel modes,
    excluding grids with no network connections.

    Parameters:
    - grid_shapefile (str): Path to the grid shapefile.
    - buffer_distance_km (int): Buffer distance in kilometers.
    - output_csv (str): Output CSV file path to save the OD pairs.

    Returns:
    - pd.DataFrame: DataFrame containing the OD pairs with travel mode ('car' or 'transit').
    """

    # Load the grid shapefile
    grids = gpd.read_file(grid_shapefile)

    # Retrieve the street network using OSMnx
    G = ox.graph_from_polygon(grids.unary_union.buffer(0.01), network_type='all')  # Buffer to avoid polygon edge issues

    # Extract nodes from the street network
    nodes = ox.graph_to_gdfs(G, nodes=True, edges=False)

    # Get the IDs of nodes in the street network
    network_node_ids = set(nodes.index)

    # Filter grids based on the presence of network connections
    grids['has_network'] = grids['geometry'].apply(lambda geom: any(node.within(geom) for node in nodes.geometry))

    # Filter out grids with no network connections
    grids_filtered = grids[grids['has_network']]

    # Initialize list to store OD pairs
    od_pairs = []

    # Generate OD pairs within the buffers for car and transit
    for travel_mode in ['car', 'transit']:
        for i, origin in grids_filtered.iterrows():
            # Create a buffer of 4 km around the origin grid
            buffer = origin.geometry.buffer(buffer_distance_km / 111.32)  # Approximate degrees per km at the equator

            for j, destination in grids_filtered.iterrows():
                if i != j and buffer.contains(destination.geometry):  # Ensure the origin and destination are not the same
                    # Calculate centroid coordinates of origin and destination
                    origin_centroid = origin.geometry.centroid
                    dest_centroid = destination.geometry.centroid

                    # Append OD pair with centroid coordinates and travel mode
                    od_pairs.append({
                        'origin_id': origin['FID'],  # Using 'FID' as the origin identifier
                        'destination_id': destination['FID'],  # Using 'FID' as the destination identifier
                        'OriginLat': float(origin_centroid.y),
                        'OriginLon': float(origin_centroid.x),
                        'DestLat': float(dest_centroid.y),
                        'DestLon': float(dest_centroid.x),
                        'travel_mode': travel_mode
                    })

    # Create a DataFrame to store the OD pairs
    od_pairs_df = pd.DataFrame(od_pairs)

    # Save the OD pairs to a CSV file
    od_pairs_df.to_csv(output_csv, index=False)

    return od_pairs_df


# Parameters
grid_shapefile = '/content/kelowna_all__grids_indicators.shp'  # Path to your grid shapefile
buffer_distance_km = 4  # Buffer distance in kilometers
output_csv = 'kelowna_od_pairs_car_transit_filtered.csv'  # Output CSV file path

# Generate OD pairs for car and transit
od_pairs_df = generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv)

print(f"Generated {len(od_pairs_df)} OD pairs (considering car and transit) within 4 km buffers from grids.")

import geopandas as gpd
import pandas as pd
import osmnx as ox
import folium
from folium.plugins import MarkerCluster
from IPython.display import display, HTML


def generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv):
    """
    Generate Origin-Destination (OD) pairs within a buffer distance from grids, considering car and transit travel modes,
    excluding grids with no network connections.

    Parameters:
    - grid_shapefile (str): Path to the grid shapefile.
    - buffer_distance_km (int): Buffer distance in kilometers.
    - output_csv (str): Output CSV file path to save the OD pairs.

    Returns:
    - pd.DataFrame: DataFrame containing the OD pairs with travel mode ('car' or 'transit').
    """

    # Load the grid shapefile
    grids = gpd.read_file(grid_shapefile)

    # Retrieve the street network using OSMnx
    G = ox.graph_from_polygon(grids.unary_union.buffer(0.01), network_type='all')  # Buffer to avoid polygon edge issues

    # Extract nodes from the street network
    nodes = ox.graph_to_gdfs(G, nodes=True, edges=False)

    # Get the IDs of nodes in the street network
    network_node_ids = set(nodes.index)

    # Filter grids based on the presence of network connections
    grids['has_network'] = grids['geometry'].apply(lambda geom: any(node.within(geom) for node in nodes.geometry))

    # Filter out grids with no network connections
    grids_filtered = grids[grids['has_network']]

    # Initialize list to store OD pairs
    od_pairs = []

    # Generate OD pairs within the buffers for car and transit
    for travel_mode in ['car', 'transit']:
        for i, origin in grids_filtered.iterrows():
            # Create a buffer of 4 km around the origin grid
            buffer = origin.geometry.buffer(buffer_distance_km / 111.32)  # Approximate degrees per km at the equator

            for j, destination in grids_filtered.iterrows():
                if i != j and buffer.contains(destination.geometry):  # Ensure the origin and destination are not the same
                    # Calculate centroid coordinates of origin and destination
                    origin_centroid = origin.geometry.centroid
                    dest_centroid = destination.geometry.centroid

                    # Append OD pair with centroid coordinates and travel mode
                    od_pairs.append({
                        'origin_id': origin['FID'],  # Using 'FID' as the origin identifier
                        'destination_id': destination['FID'],  # Using 'FID' as the destination identifier
                        'OriginLat': float(origin_centroid.y),
                        'OriginLon': float(origin_centroid.x),
                        'DestLat': float(dest_centroid.y),
                        'DestLon': float(dest_centroid.x),
                        'travel_mode': travel_mode
                    })

    # Create a DataFrame to store the OD pairs
    od_pairs_df = pd.DataFrame(od_pairs)

    # Save the OD pairs to a CSV file
    od_pairs_df.to_csv(output_csv, index=False)

    return od_pairs_df


def visualize_od_pairs(od_pairs_df, display_count=100):
    """
    Visualize a subset of OD pairs using Folium.

    Parameters:
    - od_pairs_df (pd.DataFrame): DataFrame containing the OD pairs.
    - display_count (int): Number of OD pairs to display.
    """

    # Select a subset of OD pairs for visualization
    od_pairs_subset = od_pairs_df.head(display_count)

    # Initialize the folium map centered around the mean coordinates of the origins
    folium_map = folium.Map(location=[od_pairs_subset['OriginLat'].mean(), od_pairs_subset['OriginLon'].mean()], zoom_start=12)

    # Use MarkerCluster to group markers
    marker_cluster = MarkerCluster().add_to(folium_map)

    # Add OD pairs to the map
    for _, row in od_pairs_subset.iterrows():
        folium.Marker(
            location=[row['OriginLat'], row['OriginLon']],
            icon=folium.Icon(color='red'),
            popup=f"Origin ID: {row['origin_id']}"
        ).add_to(marker_cluster)

        folium.Marker(
            location=[row['DestLat'], row['DestLon']],
            icon=folium.Icon(color='blue'),
            popup=f"Destination ID: {row['destination_id']}"
        ).add_to(marker_cluster)

        folium.PolyLine(
            locations=[(row['OriginLat'], row['OriginLon']), (row['DestLat'], row['DestLon'])],
            color='cornflowerblue',
            weight=2.5
        ).add_to(folium_map)

    # Display the map in the notebook
    display(HTML(folium_map._repr_html_()))


# Parameters
grid_shapefile = '/content/kelowna_all__grids_indicators.shp'  # Path to your grid shapefile
buffer_distance_km = 4  # Buffer distance in kilometers
output_csv = 'kelowna_od_pairs_car_transit_filtered.csv'  # Output CSV file path

# Generate OD pairs for car and transit
od_pairs_df = generate_od_pairs(grid_shapefile, buffer_distance_km, output_csv)

print(f"Generated {len(od_pairs_df)} OD pairs (considering car and transit) within 4 km buffers from grids.")

# Visualize a subset of OD pairs
visualize_od_pairs(od_pairs_df, display_count=100)

# Install Necessary Libraries
!pip install geopandas folium
!apt-get install -y libspatialindex-dev
!pip install rtree

# Read and Visualize the Shapefile
import geopandas as gpd
import folium
import matplotlib.pyplot as plt

# Path to your shapefile components
shapefile_path = '/content/kelowna_all__grids_indicators.shp'

# Read the shapefile
shapefile = gpd.read_file(shapefile_path)

# Display the attribute table
print(shapefile.head())

# Plot the shapefile using matplotlib for a static visualization
shapefile.plot()
plt.show()

# Create an interactive map using folium
# Get the centroid of the shapefile to center the map
centroid = shapefile.geometry.centroid
map_center = [centroid.y.mean(), centroid.x.mean()]
m = folium.Map(location=map_center, zoom_start=10)

# Add the shapefile to the map
folium.GeoJson(shapefile).add_to(m)

# Save and display the map
m.save('map.html')
m



# Install Necessary Libraries
!pip install geopandas folium
!apt-get install -y libspatialindex-dev
!pip install rtree

# Read and Visualize the Shapefile
import geopandas as gpd
import folium
import matplotlib.pyplot as plt
import pandas as pd
from IPython.display import display

# Path to your shapefile components
shapefile_path = '/content/kelowna_all__grids_indicators.shp'

# Read the shapefile
shapefile = gpd.read_file(shapefile_path)

# Display the attribute table
# Convert the GeoDataFrame to a DataFrame and show the first few rows
attribute_table = shapefile.drop(columns='geometry')
print(attribute_table.head())

# Display the full attribute table using IPython display
display(attribute_table)

# Plot the shapefile using matplotlib for a static visualization
shapefile.plot()
plt.show()

# Create an interactive map using folium
# Get the centroid of the shapefile to center the map
centroid = shapefile.geometry.centroid
map_center = [centroid.y.mean(), centroid.x.mean()]
m = folium.Map(location=map_center, zoom_start=10)

# Add the shapefile to the map
folium.GeoJson(shapefile).add_to(m)

# Save and display the map
m.save('map.html')
m

# Install Necessary Libraries
!pip install geopandas folium
!apt-get install -y libspatialindex-dev
!pip install rtree

# Read and Visualize the Shapefile
import geopandas as gpd
import folium
import matplotlib.pyplot as plt
import pandas as pd
from IPython.display import display
import numpy as np

# Path to your shapefile components
shapefile_path = '/content/kelowna_all__grids_indicators.shp'

# Read the shapefile
shapefile = gpd.read_file(shapefile_path)

# Display the attribute table
# Convert the GeoDataFrame to a DataFrame and show the first few rows
attribute_table = shapefile.drop(columns='geometry')
print(attribute_table.head())

# Display the full attribute table using IPython display
display(attribute_table)

# Plot the shapefile using matplotlib for a static visualization
shapefile.plot()
plt.show()

# Create an interactive map using folium
# Get the centroid of the shapefile to center the map
centroid = shapefile.geometry.centroid
map_center = [centroid.y.mean(), centroid.x.mean()]
m = folium.Map(location=map_center, zoom_start=10)

# Add the shapefile to the map
folium.GeoJson(shapefile).add_to(m)

# Save and display the map
m.save('map.html')
m

# Create a simple OD matrix using the FID as origin and destination
# Initialize an empty OD matrix
n = len(attribute_table)
OD_matrix = pd.DataFrame(np.zeros((n, n)), columns=attribute_table['FID'], index=attribute_table['FID'])

# Example: Fill the OD matrix with a sample attribute, e.g., 'street_len'
for origin in attribute_table['FID']:
    for destination in attribute_table['FID']:
        if origin != destination:
            OD_matrix.loc[origin, destination] = attribute_table.loc[origin, 'street_len']

# Display the OD matrix
display(OD_matrix)

# Install Necessary Libraries
!pip install geopandas osmnx folium
!apt-get install -y libspatialindex-dev
!pip install rtree

# Import Libraries
import geopandas as gpd
import osmnx as ox
import folium

# Path to your shapefile components
shapefile_path = '/content/kelowna_all__grids_indicators.shp'

# Read the shapefile
gdf = gpd.read_file(shapefile_path)

# Ensure Geometry Column Exists
if 'geometry' not in gdf.columns:
    raise ValueError("The shapefile does not contain a 'geometry' column.")

# Ensure that the geometries are in the same coordinate reference system
gdf = gdf.to_crs(epsg=4326)

# Create nodes at the centroids of each geometry
gdf['centroid'] = gdf.centroid

# Extract the x and y coordinates from the centroid
gdf['x'] = gdf['centroid'].x
gdf['y'] = gdf['centroid'].y

# Extract nodes from GeoDataFrame
nodes = gdf[['FID', 'x', 'y']]

# Extract edges using OSMnx
G = ox.graph_from_gdfs(nodes, gdf)

# Plot the road network
ox.plot_graph(G)

print(nodes.head())



import pandas as pd

data1= pd.read_csv('/content/Kewlona_car.csv')
data2= pd.read_csv('/content/kewlona_transit.csv')

data1

data2

# prompt: merged_data i want to save this csv

merged_data.to_csv('merged_data.csv')

import pandas as pd



# Split the 'from' and 'to' columns into separate latitude and longitude columns
def split_lat_long(df, col_name, origin_prefix, destination_prefix):
    df[[f'{origin_prefix}_x', f'{origin_prefix}_y']] = df['from'].str.split(' , ', expand=True).astype(float)
    df[[f'{destination_prefix}_x', f'{destination_prefix}_y']] = df['to'].str.split(' , ', expand=True).astype(float)

split_lat_long(data1, 'from', 'Origin', 'Destination')
split_lat_long(data2, 'from', 'Origin', 'Destination')

# Merge the dataframes on 'ID'
merged_df = pd.merge(data1, data2, on='ID', suffixes=('_car', '_pt'))

# Calculate PT/car_distance and PT/car_time
merged_df['PT/car_distance'] = merged_df['m_pt'] / merged_df['m_car']
merged_df['PT/car_time'] = merged_df['minutes_pt'] / merged_df['minutes_car']

# Select and rename the required columns
final_df = merged_df[[
    'Origin_x_car', 'Origin_y_car',
    'Destination_x_car', 'Destination_y_car',
    'm_pt', 'm_car',
    'minutes_pt', 'minutes_car',
    'PT/car_distance', 'PT/car_time'
]]

final_df.columns = [
    'Origin_x', 'Origin_y',
    'Destination_x', 'Destination_y',
    'PT_distance', 'car_distance',
    'PT_time', 'Car_time',
    'PT/car_distance', 'PT/car_time'
]

# Save to CSV
final_df.to_csv('final_data.csv', index=False)

import pandas as pd
import folium
from folium.plugins import MarkerCluster



# Split the 'from' and 'to' columns into separate latitude and longitude columns
def split_lat_long(df, origin_prefix, destination_prefix):
    df[[f'{origin_prefix}_x', f'{origin_prefix}_y']] = df['from'].str.split(' , ', expand=True).astype(float)
    df[[f'{destination_prefix}_x', f'{destination_prefix}_y']] = df['to'].str.split(' , ', expand=True).astype(float)

split_lat_long(data1, 'Origin', 'Destination')
split_lat_long(data2, 'Origin', 'Destination')

# Merge the dataframes on 'ID'
merged_df = pd.merge(data1, data2, on='ID', suffixes=('_car', '_pt'))

# Calculate PT/car_distance and PT/car_time
merged_df['PT/car_distance'] = merged_df['m_pt'] / merged_df['m_car']
merged_df['PT/car_time'] = merged_df['minutes_pt'] / merged_df['minutes_car']

# Select and rename the required columns
final_df = merged_df[[
    'Origin_x_car', 'Origin_y_car',
    'Destination_x_car', 'Destination_y_car',
    'm_pt', 'm_car',
    'minutes_pt', 'minutes_car',
    'PT/car_distance', 'PT/car_time'
]]

final_df.columns = [
    'Origin_x', 'Origin_y',
    'Destination_x', 'Destination_y',
    'PT_distance', 'car_distance',
    'PT_time', 'Car_time',
    'PT/car_distance', 'PT/car_time'
]

# Create a folium map centered around the mean coordinates of the origins
map_center = [final_df['Origin_x'].mean(), final_df['Origin_y'].mean()]
m = folium.Map(location=map_center, zoom_start=12)

# Add a marker cluster to the map
marker_cluster = MarkerCluster().add_to(m)

# Add markers and lines to the map
for _, row in final_df.iterrows():
    origin = [row['Origin_x'], row['Origin_y']]
    destination = [row['Destination_x'], row['Destination_y']]

    # Add origin marker
    folium.Marker(location=origin, popup=f"Origin: {origin}").add_to(marker_cluster)

    # Add destination marker
    folium.Marker(location=destination, popup=f"Destination: {destination}").add_to(marker_cluster)

    # Add a line between origin and destination
    folium.PolyLine(locations=[origin, destination], color='blue').add_to(m)

# Display the map
m

!pip install geopandas shapely folium

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
from google.colab import files



# Split the 'from' and 'to' columns into separate latitude and longitude columns
def split_lat_long(df, origin_prefix, destination_prefix):
    df[[f'{origin_prefix}_x', f'{origin_prefix}_y']] = df['from'].str.split(' , ', expand=True).astype(float)
    df[[f'{destination_prefix}_x', f'{destination_prefix}_y']] = df['to'].str.split(' , ', expand=True).astype(float)

split_lat_long(data1, 'Origin', 'Destination')
split_lat_long(data2, 'Origin', 'Destination')

# Merge the dataframes on 'ID'
merged_df = pd.merge(data1, data2, on='ID', suffixes=('_car', '_pt'))

# Calculate PT/car_distance and PT/car_time
merged_df['PT/car_distance'] = merged_df['m_pt'] / merged_df['m_car']
merged_df['PT/car_time'] = merged_df['minutes_pt'] / merged_df['minutes_car']

# Select and rename the required columns
final_df = merged_df[[
    'Origin_x_car', 'Origin_y_car',
    'Destination_x_car', 'Destination_y_car',
    'm_pt', 'm_car',
    'minutes_pt', 'minutes_car',
    'PT/car_distance', 'PT/car_time'
]]

final_df.columns = [
    'Origin_x', 'Origin_y',
    'Destination_x', 'Destination_y',
    'PT_distance', 'car_distance',
    'PT_time', 'Car_time',
    'PT/car_distance', 'PT/car_time'
]

# Define the CRS (WGS84)
crs = "EPSG:4326"

# Create GeoDataFrame for origins
origins_gdf = gpd.GeoDataFrame(
    final_df, geometry=gpd.points_from_xy(final_df['Origin_y'], final_df['Origin_x']), crs=crs)

# Create GeoDataFrame for destinations
destinations_gdf = gpd.GeoDataFrame(
    final_df, geometry=gpd.points_from_xy(final_df['Destination_y'], final_df['Destination_x']), crs=crs)

# Define the output file paths
origins_shapefile_path = 'origins.shp'
destinations_shapefile_path = 'destinations.shp'

# Export the GeoDataFrames to shapefiles
origins_gdf.to_file(origins_shapefile_path)
destinations_gdf.to_file(destinations_shapefile_path)

# Download the main shapefile components
files.download('origins.shp')
files.download('destinations.shp')

# Download the auxiliary shapefile components
for extension in ['cpg', 'dbf', 'prj', 'shx']:
    try:
        files.download(f'origins.{extension}')
        files.download(f'destinations.{extension}')
    except FileNotFoundError:
        print(f'File origins.{extension} or destinations.{extension} not found.')

import pandas as pd

data= pd.read_csv('/content/final_data.csv')
data

import pandas as pd

# Load the data
data = pd.read_csv('/content/final_data.csv')

# Drop rows with NaN values
data_clean = data.dropna()

# Group by 'origin_id' and calculate the mean for each group
grouped_data = data_clean.groupby('origin_id').mean().reset_index()

# Display the averaged data
grouped_data

# Save the DataFrame to a CSV file
output_file_path = 'grouped_data_processed.csv'
grouped_data.to_csv(output_file_path, index=False)

output_file_path

# prompt: files.download('grouped_data')

files.download('grouped_data')

import pandas as pd
import geopandas as gpd

# Load the CSV data
csv_path = '/content/final_data.csv'
csv_data = pd.read_csv(csv_path)

# Load the shapefile data
shapefile_path = '/content/indicator_with_unique_id.shp'
shapefile_data = gpd.read_file(shapefile_path)

# Function to scale down values that are too large
def scale_down(value, max_value=1e9):
    return value if value < max_value else max_value

# Apply scaling to the relevant columns
shapefile_data['street_den'] = shapefile_data['street_den'].apply(scale_down)
shapefile_data['edge_densi'] = shapefile_data['edge_densi'].apply(scale_down)

# Duplicate shapefile records based on the origin_id in the CSV
duplicated_shapefile_data = shapefile_data.loc[csv_data['origin_id'] - 1].reset_index(drop=True)

# Save the duplicated GeoDataFrame to a new shapefile
duplicated_shapefile_path = '/content/duplicated_indicator_with_unique_id.shp'
duplicated_shapefile_data.to_file(duplicated_shapefile_path)

print("Shapefile has been successfully duplicated and saved.")

gdf

import pandas as pd
import geopandas as gpd

# Load the CSV data
csv_path = '/content/final_data.csv'
csv_data = pd.read_csv(csv_path)

# Load the shapefile data
shapefile_path = '/content/indicator_with_unique_id.shp'
shapefile_data = gpd.read_file(shapefile_path)

# Expand the shapefile data based on the origin_id in the CSV
expanded_shapefile_data = shapefile_data.loc[csv_data['origin_id'] - 1].reset_index(drop=True)

# Ensure the unique_id matches the origin_id for merging purposes
expanded_shapefile_data['unique_id'] = csv_data['origin_id'].values

# Merge the expanded shapefile data with the CSV data
merged_data = expanded_shapefile_data.join(csv_data.set_index('origin_id'), on='unique_id', rsuffix='_csv')

# Save the updated GeoDataFrame to a new shapefile
updated_shapefile_path = '/content/expanded_indicator_with_unique_id.shp'
merged_data.to_file(updated_shapefile_path)

print("Shapefile has been successfully updated and saved.")

import pandas as pd
data= pd.read_csv('/content/final_data.csv')
data

pip install osmnx geopandas pandas



# Install necessary libraries
!pip install geopandas folium

# Import libraries
import geopandas as gpd
import folium
from folium import Choropleth

# Load the shapefile
shapefile_path = '/content/indicator.shp'
gdf = gpd.read_file(shapefile_path)

# Create a Folium map centered around the average coordinates of the shapefile
m = folium.Map(location=[gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean()], zoom_start=12)

# Add the Choropleth layer to the map
Choropleth(
    geo_data=gdf,
    name='choropleth',
    data=gdf,
    columns=['Id', 'PT_car_tim'],
    key_on='feature.properties.Id',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='PT_car_dis'
).add_to(m)

# Add layer control to toggle between layers
folium.LayerControl().add_to(m)

# Save the map to an HTML file and display it
m.save('map.html')
m

import geopandas as gpd

# Load the Shapefile
gdf = gpd.read_file("merged_output.shp")

# # Add a unique ID column
# gdf = gdf.assign(unique_id=range(1, len(gdf) + 1))

# Display the updated DataFrame with the unique ID
print(gdf.head())

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point

# Step 1: Read the CSV file
csv_file_path = '/content/final_data.csv'
dt = pd.read_csv(csv_file_path)

# Step 2: Load the Shapefile and add unique IDs if not already present
shapefile_path = "indicator.shp"
gdf = gpd.read_file(shapefile_path)
gdf = gdf.assign(unique_id=range(1, len(gdf) + 1))

# Step 3: Merge the unique ID and all relevant indicator values into the original DataFrame
# Ensure all relevant indicator columns are included in the merge
indicator_columns = ['unique_id', 'origin_id', 'circuity', 'street_seg', 'street_len',
                     'streets_pe', 'edge_lengt', 'street_den', 'edge_densi', 'node_densi']
merged_df = pd.merge(dt, gdf[indicator_columns], on='origin_id', how='left')

# Step 4: Create geometry column based on 'Origin_x' and 'Origin_y'
merged_df['geometry'] = merged_df.apply(lambda row: Point(row['Origin_x'], row['Origin_y']), axis=1)

# Step 5: Convert to GeoDataFrame
merged_gdf = gpd.GeoDataFrame(merged_df, geometry='geometry')

# Step 6: Save the GeoDataFrame to a new Shapefile
output_shapefile_path = "merged_output.shp"
merged_gdf.to_file(output_shapefile_path, driver='ESRI Shapefile')

print("Shapefile saved successfully!")

# Step 1: Install necessary libraries (if not already installed)
!pip install geopandas scikit-learn matplotlib

# Step 2: Load the dataset and prepare the data
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns  # For heatmap visualization
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Adjust display settings
pd.set_option('display.max_columns', None)

# Load the Shapefile (ensure the file name matches the uploaded .shp file)
gdf = gpd.read_file("data.shp")

# Select features and target variable
features = ['circuity', 'street_len', 'streets_pe', 'edge_lengt', 'street_den', 'edge_densi', 'node_densi']
target = 'PT_car_dis'

X = gdf[features]
y = gdf[target]

# Check for any missing values and handle them (e.g., drop or fill)
X = X.dropna()
y = y.dropna()

# Ensure X and y have the same length after dropping missing values
X = X.loc[y.index]

# Step 3: Check for multicollinearity
correlation_matrix = X.corr()

# Step 4: Visualize the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# Step 1: Install necessary libraries (if not already installed)
!pip install geopandas scikit-learn matplotlib seaborn

# Step 2: Load the dataset and prepare the data
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns  # For heatmap visualization
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Adjust display settings
pd.set_option('display.max_columns', None)

# Load the Shapefile (ensure the file name matches the uploaded .shp file)
gdf = gpd.read_file("data.shp")

# Select features and target variable
features = [ 'intersecti', 'circuity', 'street_seg', 'street_len',
 'streets_pe', 'edge_lengt', 'street_den', 'edge_densi', 'node_densi',
 'one_way_in', 'two_way_in', 'four_way_i', 'avg_betwee', 'avg_closen']

target = 'PT_car_dis'

X = gdf[features]
y = gdf[target]

# Check for any missing values and handle them (e.g., drop or fill)
X = X.dropna()
y = y.dropna()

# Ensure X and y have the same length after dropping missing values
X = X.loc[y.index]

# Step 3: Calculate the correlation matrix
correlation_matrix = X.corr()

# Step 4: Print the correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)



# Print the coefficients
coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_})
print("Coefficients:")
print(coefficients)

# Extract the intercept
intercept = model.intercept_
print(f"\nIntercept: {intercept}\n")

# Print the regression equation
equation_parts = []
equation_parts.append(f"PT_car_dis = {intercept:.2f}")
for feature, coef in zip(X.columns, model.coef_):
    equation_parts.append(f" + {coef:.2f} * {feature}")

regression_equation = ''.join(equation_parts)
print("Regression Equation:")
print(regression_equation)



import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
import statsmodels.api as sm
import pysal.lib
from pysal.model import spreg
from esda.moran import Moran
from splot.esda import moran_scatterplot
from splot.libpysal import plot_spatial_weights

# Load the dataset
gdf = gpd.read_file("data.shp")

# Project the shapefile to UTM 45N
gdf = gdf.to_crs(epsg=32645)

# Create spatial weights matrix using Queen contiguity
w = pysal.lib.weights.Queen.from_dataframe(gdf)

# Select features and target variable
features = ['circuity', 'streets_pe', 'edge_lengt', 'one_way_in', 'two_way_in', 'avg_betwee', 'avg_closen']
target = 'PT_car_tim'

# Ensure features and target are in the DataFrame
data = gdf[features + [target]]

# Drop rows with missing values
data = data.dropna()

# Separate features and target variable
X = data[features]
y = data[target]

# Apply logarithmic transformation to highly skewed features
X_log = X.copy()
for feature in features:
    if X[feature].skew() > 1:
        X_log[feature] = np.log1p(X[feature])  # log1p to handle zero values

# Add interaction terms
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X_log)
poly_feature_names = poly.get_feature_names_out(features)

# Normalize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_poly)

# Fit spatial regression model (Spatial Lag Model)
model = spreg.GM_Lag(y.values, X_scaled, w=w, name_y=target, name_x=poly_feature_names.tolist())

# Print regression results
print(model.summary)

# Extract coefficients and print the regression equation
intercept = model.betas[0][0]
coefficients = model.betas[1:]

regression_equation = f"{target} = {intercept:.3f}"
for idx, coef in enumerate(coefficients):
    if idx < len(poly_feature_names):
        regression_equation += f" + {coef[0]:.3f}*{poly_feature_names[idx]}"
    else:
        regression_equation += f" + {coef[0]:.3f}*W_{target}"

print("Spatial Regression Equation:")
print(regression_equation)

# Calculate Moran's I for the target variable
mi = Moran(y.values, w)
print(f"Moran's I: {mi.I}, p-value: {mi.p_norm}")

# Plot Moran's I scatter plot
fig, ax = plt.subplots(1, 1, figsize=(8, 6))
moran_scatterplot(mi, ax=ax)
plt.title("Moran's I Scatter Plot")
plt.show()

# Visualize the contiguity matrix
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
plot_spatial_weights(w, gdf, ax=ax)
plt.title('Queen Contiguity Spatial Weights')
plt.show()

# Optional: Visualize residuals
pred_y = model.predy.flatten()  # Ensure predy is 1-dimensional
residuals = y - pred_y

plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.title('Residual Distribution')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()

import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from libpysal.weights import Queen
from esda.moran import Moran_Local
from spreg import GWR, ML_Error
from spreg.utils import set_endog, set_exog

# Load the dataset
gdf = gpd.read_file("data.shp")

# Project the shapefile to UTM 45N
gdf = gdf.to_crs(epsg=32645)

# Select features and target variable
features = ['circuity', 'streets_pe', 'edge_lengt', 'one_way_in', 'two_way_in', 'avg_betwee', 'avg_closen']
target = 'PT_car_tim'

# Ensure features and target are in the DataFrame
data = gdf[features + [target]]

# Drop rows with missing values
data = data.dropna()

# Separate features and target variable
X = data[features]
y = data[target]

# Apply logarithmic transformation to highly skewed features
X_log = X.copy()
for feature in features:
    if X[feature].skew() > 1:
        X_log[feature] = np.log1p(X[feature])  # log1p to handle zero values

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_log)

# Create spatial weights matrix using Queen contiguity
w = Queen.from_dataframe(gdf)

# Fit GWR model
gwr_model = GWR(y, X, w)

# Print summary of GWR model
print(gwr_model.summary())

# Plot coefficient maps
fig, axs = plt.subplots(3, 3, figsize=(15, 15))
for i, ax in enumerate(axs.flat):
    gwr_model.plot_coefficients(i, ax=ax)
plt.suptitle('GWR Coefficient Maps')
plt.tight_layout()
plt.show()

# Plot diagnostics
fig, axs = plt.subplots(1, 3, figsize=(18, 6))
gwr_model.plot_residuals(ax=axs[0])
gwr_model.plot_local_r_squared(ax=axs[1])
moran_loc = Moran_Local(gwr_model.resid_response, w)
plot_local_autocorrelation(moran_loc, gdf, 'PT_car_tim', ax=axs[2])
plt.suptitle('GWR Diagnostics')
plt.tight_layout()
plt.show()

import pandas as pd
import geopandas as gpd
import osmnx as ox
from shapely.geometry import Point

# Step 1: Load CSV data into DataFrame
df = pd.read_csv('/content/final_data1.csv')

# Step 2: Create GeoDataFrame with Point geometries for origins and destinations
# Explicitly set CRS to EPSG:4326 (WGS84)
gdf_origins = gpd.GeoDataFrame(
    df, geometry=gpd.points_from_xy(df['Origin_x'], df['Origin_y']), crs='EPSG:4326')
gdf_destinations = gpd.GeoDataFrame(
    df, geometry=gpd.points_from_xy(df['Destination_x'], df['Destination_y']), crs='EPSG:4326')

# Step 3: Function to calculate indicators for each origin-destination pair
def calculate_osm_indicators(origin, destination):
    try:
        # Create a graph from origin to destination
        G = ox.graph_from_point(origin, dist=1000, network_type='drive')

        # Calculate basic stats
        stats = ox.basic_stats(G)

        # Return indicators
        return {
            'intersection_count': len(G.nodes),
            'circuity': stats['circuity_avg'],
            'street_segment_count': len(G.edges),
            'total_street_length': stats['street_length_total'],
            'streets_per_node_avg': stats['streets_per_node_avg']
        }
    except Exception as e:
        print(f"Error calculating indicators for {origin} -> {destination}: {e}")
        return None

# Step 4: Calculate indicators for each row (origin-destination pair)
indicators = []
for index, row in df.iterrows():
    origin = (row['Origin_y'], row['Origin_x'])
    destination = (row['Destination_y'], row['Destination_x'])

    # Calculate indicators
    indicators.append(calculate_osm_indicators(origin, destination))

# Step 5: Add indicators to DataFrame
df['intersection_count'] = [ind['intersection_count'] if ind else None for ind in indicators]
df['circuity'] = [ind['circuity'] if ind else None for ind in indicators]
df['street_segment_count'] = [ind['street_segment_count'] if ind else None for ind in indicators]
df['total_street_length'] = [ind['total_street_length'] if ind else None for ind in indicators]
df['streets_per_node_avg'] = [ind['streets_per_node_avg'] if ind else None for ind in indicators]

# Step 6: Display the updated DataFrame with OSM indicators
print(df.head())

import pandas as pd

data = pd.read_csv('/content/final_data.csv')
data.head()

import pandas as pd
import folium
from folium import Marker

# Load data
data = pd.read_csv('/content/final_data.csv')

# Calculate center of the map based on mean coordinates
center_lat = data[['Origin_y', 'Destination_y']].mean().mean()
center_lon = data[['Origin_x', 'Destination_x']].mean().mean()

# Create a Folium map centered around the mean coordinates
m = folium.Map(location=[center_lat, center_lon], zoom_start=12)

# Add markers for origins and destinations
for idx, row in data.iterrows():
    origin = (row['Origin_y'], row['Origin_x'])
    destination = (row['Destination_y'], row['Destination_x'])

    # Add markers for origins and destinations
    Marker(location=origin, popup=f"Origin {row['origin_id']}").add_to(m)
    Marker(location=destination, popup=f"Destination {row['destination_id']}").add_to(m)

# Display the map
m

pip install --upgrade geopandas pandas networkx numpy requests shapely fiona pyproj

import pandas as pd
data= pd.read_csv('/content/merged_final_data.csv')
data.head()

import pandas as pd
import numpy as np

# Load the merged dataset
merged_csv_path = '/content/merged_final_data.csv'  # Replace with your actual path
df = pd.read_csv(merged_csv_path)

# Display the first few rows of the DataFrame
print("First few rows of the DataFrame:")
print(df.head())

# Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# Drop rows with missing values (if any)
df_cleaned = df.dropna()

# Display the first few rows after cleaning
print("\nFirst few rows after cleaning:")
print(df_cleaned.head())

# Select relevant columns for correlation and regression analysis
# Adjust the column list based on your specific analysis needs
relevant_columns = [
    'PT/car_distance', 'PT/car_time', 'intersecti', 'circuity', 'street_seg',
    'street_len', 'streets_pe', 'edge_lengt', 'street_den', 'edge_densi',
    'node_densi', 'one_way_in', 'two_way_in', 'four_way_i', 'avg_betwee',
    'avg_closen', 'PT_car_dis', 'PT_car_tim'
]

df_relevant = df_cleaned[relevant_columns]

# Check data types
print("\nData types of relevant columns:")
print(df_relevant.dtypes)

# Calculate the correlation matrix
correlation_matrix = df_relevant.corr()

# Display the correlation matrix
print("\nCorrelation matrix:")
print(correlation_matrix)

# Save the correlation matrix to a CSV file
correlation_matrix_path = '/content/correlation_matrix.csv'  # Replace with your desired path
correlation_matrix.to_csv(correlation_matrix_path)
print(f"\nCorrelation matrix saved to {correlation_matrix_path}")

# # Prepare data for regression
# # Define the dependent variable (Y) and independent variables (X)
# # For example, let's predict 'PT/car_time' based on other variables
# X = df_relevant.drop(columns='PT/car_time')
# y = df_relevant['PT/car_time']

# # Save the prepared dataset for regression
# prepared_dataset_path = '/content/prepared_for_regression.csv'  # Replace with your desired path
# df_relevant.to_csv(prepared_dataset_path, index=False)
# print(f"\nPrepared dataset for regression saved to {prepared_dataset_path}")

# # Example code to perform a simple linear regression
# import statsmodels.api as sm

# # Add a constant term for the intercept
# X = sm.add_constant(X)

# # Fit the regression model
# model = sm.OLS(y, X).fit()

# # Display the regression results
# print("\nRegression results summary:")
# print(model.summary())

import pandas as pd
import numpy as np
import statsmodels.api as sm

# Load the cleaned and prepared dataset
merged_csv_path = '/content/merged_final_data.csv'  # Replace with your actual path
df = pd.read_csv(merged_csv_path)
df
# # Select relevant columns for the analysis
# relevant_columns = [
#     # 'intersecti',
#     #  'circuity',
#     # 'street_seg',
#     # 'street_len', 'streets_pe',
#      'edge_lengt',
#     'street_den',
#     'edge_densi', 'node_densi',
#     'avg_betwee',
#     'avg_closen'
# ]

# # Define the independent variables (X) and the dependent variable (y)
# X = df[relevant_columns]
# y = df['PT_car_dis']

# # Replace infinite values with NaN
# X.replace([np.inf, -np.inf], np.nan, inplace=True)
# y.replace([np.inf, -np.inf], np.nan, inplace=True)

# # Drop rows with NaN values in both X and y
# X.dropna(inplace=True)
# y.dropna(inplace=True)

# # Add a constant term for the intercept
# X = sm.add_constant(X)




# # Fit the regression model
# model = sm.OLS(y, X).fit()

# # Display the regression results with interaction terms
# print("\nRegression results summary with interaction terms:")
# print(model.summary())

# # Save the regression results summary to a text file
# regression_results_path = '/content/regression_results_summary_with_interactions.txt'  # Replace with your desired path
# with open(regression_results_path, 'w') as f:
#     f.write(model.summary().as_text())
# print(f"\nRegression results summary with interaction terms saved to {regression_results_path}")

!pip install geopandas

!pip install pysal
!pip install spreg

import pandas as pd
import geopandas as gpd
from libpysal import weights
from spreg import ML_Lag_Reg

# Step 1: Load your dataset
# Assuming your dataset is already loaded as a DataFrame 'df'
# Replace this with your actual dataset loading method

Example: Loading from a CSV file
df = pd.read_csv('/content/merged_final_data.csv')

# Step 2: Create a GeoDataFrame and set the geometry
geometry = gpd.points_from_xy(df['Origin_x_x'], df['Origin_y_x'])
gdf = gpd.GeoDataFrame(df, geometry=geometry)

# Step 3: Drop rows with missing values in target or predictors
gdf.dropna(subset=['PT_car_dis', 'intersecti', 'circuity', 'street_den', 'edge_densi',
                    'node_densi', 'one_way_in', 'two_way_in', 'four_way_i',
                    'avg_betwee', 'avg_closen'], inplace=True)

# Step 4: Create spatial weights matrix
w = weights.KNN.from_dataframe(gdf, k=5)  # Adjust 'k' as needed

# Step 5: Define dependent and independent variables
y = gdf['PT_car_dis'].values
X = gdf[['intersecti', 'circuity', 'street_den', 'edge_densi',
         'node_densi', 'one_way_in', 'two_way_in', 'four_way_i',
         'avg_betwee', 'avg_closen']].values

# Step 6: Fit Ridge regression model
ridge_model = ML_Lag_Reg(y, X, w=w, name_y='PT_car_dis', name_x=['intersecti', 'circuity',
                                                                 'street_den', 'edge_densi',
                                                                 'node_densi', 'one_way_in',
                                                                 'two_way_in', 'four_way_i',
                                                                 'avg_betwee', 'avg_closen'],
                         name_w='knn', method='ord', name_ds='your_dataset_name')

# Step 7: Print summary
print(ridge_model.summary)

# Optional: Additional analysis or plotting
# Example: Plotting residuals
import matplotlib.pyplot as plt

plt.scatter(gdf['geometry'].x, gdf['geometry'].y, c=ridge_model.u)
plt.colorbar()
plt.title('Spatial Residuals')
plt.show()

# Extract the top indicators of major importance
top_indicators = results_summary.head(2)  # Adjust the number as needed

print("\nTop indicators of major importance:")
print(top_indicators)